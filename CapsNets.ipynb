{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahes\\AppData\\Local\\Temp\\ipykernel_15396\\2943534503.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoname</th>\n",
       "      <th>original_width</th>\n",
       "      <th>original_height</th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aznyksihgl.mp4</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>xnojggkrxt.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gkwmalrvcj.mp4</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>hqqmtxvbjj.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lxnqzocgaq.mp4</td>\n",
       "      <td>223</td>\n",
       "      <td>217</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>xjzkfqddyk.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itsbtrrelv.mp4</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kqvepwqxfe.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ddvgrczjno.mp4</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>pluadmqqta.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        videoname  original_width  original_height label        original\n",
       "0  aznyksihgl.mp4             129              129  FAKE  xnojggkrxt.mp4\n",
       "1  gkwmalrvcj.mp4             129              129  FAKE  hqqmtxvbjj.mp4\n",
       "2  lxnqzocgaq.mp4             223              217  FAKE  xjzkfqddyk.mp4\n",
       "3  itsbtrrelv.mp4             186              186  FAKE  kqvepwqxfe.mp4\n",
       "4  ddvgrczjno.mp4             155              155  FAKE  pluadmqqta.mp4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "# Set up visualization settings\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "## Data Visualization\n",
    "\n",
    "import os\n",
    "\n",
    "# Function to get data\n",
    "def get_data():\n",
    "    return pd.read_csv('metadata.csv/metadata.csv')\n",
    "\n",
    "meta = get_data()\n",
    "meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8960, 5), (3840, 5), (3200, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducing dataset size to 16000 images\n",
    "sample_size = 16000\n",
    "\n",
    "real_df = meta[meta[\"label\"] == \"REAL\"].sample(sample_size // 2, random_state=42)\n",
    "fake_df = meta[meta[\"label\"] == \"FAKE\"].sample(sample_size // 2, random_state=42)\n",
    "\n",
    "sample_meta = pd.concat([real_df, fake_df])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set, Test_set = train_test_split(sample_meta, test_size=0.2, random_state=42, stratify=sample_meta['label'])\n",
    "Train_set, Val_set = train_test_split(Train_set, test_size=0.3, random_state=42, stratify=Train_set['label'])\n",
    "\n",
    "Train_set.shape, Val_set.shape, Test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataset(set_name):\n",
    "    images, labels = [], []\n",
    "    for (img, imclass) in zip(set_name['videoname'], set_name['label']):\n",
    "        images.append(cv2.imread('faces_224/' + img[:-4] + '.jpg'))\n",
    "        labels.append(1 if imclass == 'FAKE' else 0)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "X_train, y_train = retrieve_dataset(Train_set)\n",
    "X_val, y_val = retrieve_dataset(Val_set)\n",
    "X_test, y_test = retrieve_dataset(Test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"capsule_layer_4\" (type CapsuleLayer).\n\nin user code:\n\n    File \"C:\\Users\\mahes\\AppData\\Local\\Temp\\ipykernel_15396\\1877425202.py\", line 23, in call  *\n        inputs_tiled = tf.tile(inputs_expand, [1, 1, self.num_capsules, 1])\n\n    ValueError: Shape must be rank 5 but is rank 4 for '{{node capsule_layer_4/Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](capsule_layer_4/ExpandDims, capsule_layer_4/Tile/multiples)' with input shapes: [?,2,2,1,256], [4].\n\n\nCall arguments received by layer \"capsule_layer_4\" (type CapsuleLayer):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mahes\\ML\\resarch_paper\\CapsNets.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahes/ML/resarch_paper/CapsNets.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m x \u001b[39m=\u001b[39m base_model\u001b[39m.\u001b[39moutput\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahes/ML/resarch_paper/CapsNets.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m x \u001b[39m=\u001b[39m Conv2D(filters\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mahes/ML/resarch_paper/CapsNets.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m x \u001b[39m=\u001b[39m CapsuleLayer()(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahes/ML/resarch_paper/CapsNets.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m Flatten()(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mahes/ML/resarch_paper/CapsNets.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m Dense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "File \u001b[1;32mc:\\Users\\mahes\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetsgs30eb.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m inputs_expand \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mexpand_dims, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), fscope)\n\u001b[1;32m---> 11\u001b[0m inputs_tiled \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mtile, (ag__\u001b[39m.\u001b[39mld(inputs_expand), [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_capsules, \u001b[39m1\u001b[39m]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m caps \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmatmul, (ag__\u001b[39m.\u001b[39mld(inputs_tiled), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mW), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m caps \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(caps), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(caps)\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_capsules, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdim_capsule)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"capsule_layer_4\" (type CapsuleLayer).\n\nin user code:\n\n    File \"C:\\Users\\mahes\\AppData\\Local\\Temp\\ipykernel_15396\\1877425202.py\", line 23, in call  *\n        inputs_tiled = tf.tile(inputs_expand, [1, 1, self.num_capsules, 1])\n\n    ValueError: Shape must be rank 5 but is rank 4 for '{{node capsule_layer_4/Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](capsule_layer_4/ExpandDims, capsule_layer_4/Tile/multiples)' with input shapes: [?,2,2,1,256], [4].\n\n\nCall arguments received by layer \"capsule_layer_4\" (type CapsuleLayer):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Capsule Layer\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, num_capsules=10, dim_capsule=16, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(CapsuleLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expand = tf.expand_dims(inputs, axis=-2)\n",
    "        inputs_tiled = tf.tile(inputs_expand, [1, 1, self.num_capsules, 1])\n",
    "        caps = tf.matmul(inputs_tiled, self.W)\n",
    "        caps = tf.reshape(caps, (-1, caps.shape[1], self.num_capsules, self.dim_capsule))\n",
    "        return caps\n",
    "\n",
    "# Build Capsule Network using InceptionResNetV2 base\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze InceptionResNetV2 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Capsule Network architecture\n",
    "x = base_model.output\n",
    "x = Conv2D(filters=256, kernel_size=3, strides=2, activation='relu')(x)\n",
    "x = CapsuleLayer()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the Capsule Network model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
